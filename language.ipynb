{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Dmsbq-1OiurAEhN_-Yk63kl3oGvLQYSQ",
      "authorship_tag": "ABX9TyM43J8W3Wmz5GZBkaR4oy2r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/Deeplearning-for-everyone/blob/main/language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://www.example.com'\n",
        "response = requests.get(url)\n",
        "text_data = response.text\n",
        "\n",
        "# 텍스트 데이터를 파일로 저장\n",
        "with open('text_data.txt', 'w') as f:\n",
        "    f.write(text_data)\n"
      ],
      "metadata": {
        "id": "EJMmelM7vs8R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWcIrTv0Fegc",
        "outputId": "3a21a96d-341a-4af5-eacf-6fde22eee2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# 텍스트 데이터 로드\n",
        "with open('text_data.txt', 'r') as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# 소문자 변환\n",
        "text_data = text_data.lower()\n",
        "\n",
        "# 구두점 및 불필요한 공백 제거\n",
        "text_data = re.sub('[%s]' % re.escape(string.punctuation), '', text_data)\n",
        "text_data = re.sub('\\s+', ' ', text_data)\n",
        "\n",
        "# 단어 토큰화\n",
        "nltk.download('punkt')\n",
        "tokens = nltk.word_tokenize(text_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 텍스트 데이터 로드\n",
        "with open('text_data.txt', 'r') as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# Tokenizer를 사용하여 단어를 숫자로 변환\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "sequences = tokenizer.texts_to_sequences([text_data])[0]\n",
        "\n",
        "# 입력 시퀀스 생성\n",
        "seq_length = 50\n",
        "sequences = tf.keras.preprocessing.sequence.pad_sequences([sequences], maxlen=seq_length+1, padding='pre')\n",
        "\n",
        "# 입력 시퀀스와 타깃 시퀀스 생성\n",
        "input_sequences = sequences[:, :-1]\n",
        "target_sequences = sequences[:, 1:]\n",
        "\n",
        "# 클래스의 개수 정의\n",
        "num_classes = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 언어 모델 아키텍처 정의\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences=True, input_shape=(50, 1)))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 모델 학습\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "input_sequences = input_sequences.reshape(-1, 50, 1)\n",
        "model.fit(input_sequences, tf.keras.utils.to_categorical(target_sequences, num_classes=num_classes), batch_size=128, epochs=100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDUSKwaaz9Fw",
        "outputId": "88948033-90db-4082-ea83-086137ac12af"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 7s 7s/step - loss: 4.5693 - accuracy: 0.0200\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 4.4777 - accuracy: 0.0200\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 4.3938 - accuracy: 0.0600\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 4.3091 - accuracy: 0.0600\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 4.2211 - accuracy: 0.0800\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 4.1324 - accuracy: 0.0800\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 4.0464 - accuracy: 0.0800\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 3.9660 - accuracy: 0.0800\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 3.8924 - accuracy: 0.0800\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 3.8251 - accuracy: 0.0800\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.7628 - accuracy: 0.0800\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 3.7030 - accuracy: 0.0600\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 3.6452 - accuracy: 0.1000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 3.5886 - accuracy: 0.1000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 3.5380 - accuracy: 0.1200\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 3.4928 - accuracy: 0.1200\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 3.4526 - accuracy: 0.1000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 3.4243 - accuracy: 0.1200\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 3.3828 - accuracy: 0.1200\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 3.3492 - accuracy: 0.1400\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 3.3164 - accuracy: 0.1600\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 3.2966 - accuracy: 0.1400\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 3.2556 - accuracy: 0.1600\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 3.2360 - accuracy: 0.1600\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 3.1964 - accuracy: 0.1400\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 3.1777 - accuracy: 0.1200\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 3.1470 - accuracy: 0.1600\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 3.1268 - accuracy: 0.1600\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 3.1011 - accuracy: 0.1600\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 3.0754 - accuracy: 0.1600\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 3.0545 - accuracy: 0.1400\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 3.0262 - accuracy: 0.1400\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 2.9985 - accuracy: 0.1600\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 2.9810 - accuracy: 0.1800\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.9638 - accuracy: 0.2000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.9345 - accuracy: 0.1800\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.9155 - accuracy: 0.2000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 2.8930 - accuracy: 0.1800\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.8718 - accuracy: 0.1800\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.8513 - accuracy: 0.1800\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.8307 - accuracy: 0.1800\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.8327 - accuracy: 0.1800\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 2.8621 - accuracy: 0.1400\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 2.7739 - accuracy: 0.1800\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.8031 - accuracy: 0.2000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 2.7661 - accuracy: 0.1800\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.7490 - accuracy: 0.1600\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.7240 - accuracy: 0.2000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.7145 - accuracy: 0.1800\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 2.6802 - accuracy: 0.1600\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 2.6789 - accuracy: 0.1800\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 2.6425 - accuracy: 0.2000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 2.6380 - accuracy: 0.2000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.6202 - accuracy: 0.2200\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 2.5969 - accuracy: 0.2000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.5952 - accuracy: 0.1800\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 2.5609 - accuracy: 0.2000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 2.5647 - accuracy: 0.2000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.5279 - accuracy: 0.2200\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.5275 - accuracy: 0.2600\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.4989 - accuracy: 0.2400\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.4932 - accuracy: 0.2400\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 2.4656 - accuracy: 0.2400\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.4672 - accuracy: 0.2400\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 2.4392 - accuracy: 0.2400\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 2.4255 - accuracy: 0.2200\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 2.4256 - accuracy: 0.2200\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 2.3948 - accuracy: 0.2400\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.3857 - accuracy: 0.2600\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 2.3737 - accuracy: 0.2400\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 2.3537 - accuracy: 0.2400\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.3480 - accuracy: 0.2600\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 2.3308 - accuracy: 0.2400\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.3152 - accuracy: 0.2400\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.3065 - accuracy: 0.2600\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 2.2993 - accuracy: 0.2200\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 2.2800 - accuracy: 0.2800\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 2.2674 - accuracy: 0.2600\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 2.2587 - accuracy: 0.2200\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 2.2478 - accuracy: 0.3000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 2.2329 - accuracy: 0.2400\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.2189 - accuracy: 0.2800\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 2.2069 - accuracy: 0.2800\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 2.1977 - accuracy: 0.2800\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 2.1887 - accuracy: 0.3200\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 2.1809 - accuracy: 0.2400\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 2.1700 - accuracy: 0.3400\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 2.1576 - accuracy: 0.2800\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 2.1424 - accuracy: 0.3000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.1310 - accuracy: 0.3000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 2.1227 - accuracy: 0.2800\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 2.1182 - accuracy: 0.3600\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 2.1165 - accuracy: 0.2800\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.1043 - accuracy: 0.3800\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 2.0854 - accuracy: 0.3000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.0710 - accuracy: 0.3800\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.0650 - accuracy: 0.3800\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 2.0560 - accuracy: 0.3000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 2.0471 - accuracy: 0.3800\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 2.0370 - accuracy: 0.3200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6637cea90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 텍스트 데이터 로드\n",
        "with open('text_data.txt', 'r') as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "# Tokenizer를 사용하여 단어를 숫자로 변환\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text_data])\n",
        "sequences = tokenizer.texts_to_sequences([text_data])[0]\n",
        "\n",
        "# 입력 시퀀스 생성\n",
        "seq_length = 50\n",
        "sequences = tf.keras.preprocessing.sequence.pad_sequences([sequences], maxlen=seq_length+1, padding='pre')\n",
        "\n",
        "# 입력 시퀀스와 타깃 시퀀스 생성\n",
        "input_sequences = sequences[:, :-1]\n",
        "target_sequences = sequences[:, 1:]\n",
        "\n",
        "# 클래스의 개수 정의\n",
        "num_classes = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 언어 모델 아키텍처 정의\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences=True, input_shape=(50, 1)))\n",
        "model.add(Dropout(0.2)) # dropout 적용\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2)) # dropout 적용\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2)) # dropout 적용\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 모델 학습\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "input_sequences = input_sequences.reshape(-1, 50, 1)\n",
        "model.fit(input_sequences, tf.keras.utils.to_categorical(target_sequences, num_classes=num_classes), batch_size=128, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8QT7vDXfnR",
        "outputId": "1a2d30c4-c484-436e-ee84-ae10a2e37975"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 6s 6s/step - loss: 4.5656 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 4.5282 - accuracy: 0.0200\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 4.4778 - accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 4.3946 - accuracy: 0.0400\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 4.3337 - accuracy: 0.0400\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 4.3066 - accuracy: 0.1000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 4.2613 - accuracy: 0.0600\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 4.2025 - accuracy: 0.0200\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 4.1326 - accuracy: 0.0400\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 4.0422 - accuracy: 0.0200\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 4.0282 - accuracy: 0.0800\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 4.0053 - accuracy: 0.0600\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 3.8469 - accuracy: 0.1200\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 3.8458 - accuracy: 0.1000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 3.7754 - accuracy: 0.0800\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 3.7623 - accuracy: 0.0400\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 3.6993 - accuracy: 0.0600\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 3.6487 - accuracy: 0.0600\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 3.6294 - accuracy: 0.0600\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.5681 - accuracy: 0.1200\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 3.5053 - accuracy: 0.1000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 3.4981 - accuracy: 0.0800\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 3.4784 - accuracy: 0.1000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 3.3694 - accuracy: 0.1600\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 3.3491 - accuracy: 0.1600\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 3.4360 - accuracy: 0.1000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 3.2370 - accuracy: 0.1400\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 3.2378 - accuracy: 0.0800\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 3.2715 - accuracy: 0.0600\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 3.2324 - accuracy: 0.1200\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 3.2060 - accuracy: 0.1200\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 3.1784 - accuracy: 0.1800\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 3.1545 - accuracy: 0.1200\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 3.2047 - accuracy: 0.1200\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 3.1398 - accuracy: 0.1200\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 3.1209 - accuracy: 0.1200\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 3.0889 - accuracy: 0.1600\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 3.1172 - accuracy: 0.1400\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 3.0140 - accuracy: 0.1800\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 2.9545 - accuracy: 0.1400\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 3.0519 - accuracy: 0.1200\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 2.9945 - accuracy: 0.1800\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 2.9870 - accuracy: 0.1400\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 3.0367 - accuracy: 0.1600\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 3.0266 - accuracy: 0.1600\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 3.0146 - accuracy: 0.1200\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 2.9218 - accuracy: 0.1200\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 2.9435 - accuracy: 0.1400\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 2.8668 - accuracy: 0.1600\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 2.8866 - accuracy: 0.1800\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 2.8514 - accuracy: 0.1600\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.9117 - accuracy: 0.1000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.8123 - accuracy: 0.2400\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.9332 - accuracy: 0.1200\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 2.7997 - accuracy: 0.1600\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 2.8047 - accuracy: 0.1000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 2.8588 - accuracy: 0.1000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 2.8292 - accuracy: 0.1600\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 2.6405 - accuracy: 0.2800\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.7271 - accuracy: 0.1600\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 2.7656 - accuracy: 0.2200\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 2.8057 - accuracy: 0.1200\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 2.7573 - accuracy: 0.1600\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 2.7453 - accuracy: 0.1200\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.6984 - accuracy: 0.1400\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.7230 - accuracy: 0.1400\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 2.8085 - accuracy: 0.1600\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 2.7900 - accuracy: 0.1200\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 2.7598 - accuracy: 0.1800\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 2.7592 - accuracy: 0.1800\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 2.8001 - accuracy: 0.1600\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 2.6775 - accuracy: 0.2000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 2.6932 - accuracy: 0.1400\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 2.7148 - accuracy: 0.1400\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 2.6553 - accuracy: 0.2200\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 2.6320 - accuracy: 0.2000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 2.6973 - accuracy: 0.1600\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 2.6109 - accuracy: 0.2200\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 2.6319 - accuracy: 0.2000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.5790 - accuracy: 0.2200\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 2.5307 - accuracy: 0.2600\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.5805 - accuracy: 0.2000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 2.5188 - accuracy: 0.1800\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 2.5474 - accuracy: 0.1600\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 2.4956 - accuracy: 0.2000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 2.4696 - accuracy: 0.2200\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 2.5149 - accuracy: 0.1600\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 2.5665 - accuracy: 0.1600\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 2.4220 - accuracy: 0.2400\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 2.4878 - accuracy: 0.2800\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.4770 - accuracy: 0.1800\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.4539 - accuracy: 0.1600\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.4159 - accuracy: 0.2600\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.5395 - accuracy: 0.1600\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 2.4250 - accuracy: 0.2400\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 2.3767 - accuracy: 0.2400\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 2.4081 - accuracy: 0.2600\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 2.3377 - accuracy: 0.2000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 2.3796 - accuracy: 0.1800\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 2.3987 - accuracy: 0.2600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6657e6550>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bymsz4vwY9ZP",
        "outputId": "0a89a6a8-9f21-4e9c-b9f7-fa2e907f4abf"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<keras.engine.sequential.Sequential object at 0x7fb66586ac40>\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_42 (LSTM)              (None, 50, 100)           40800     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 50, 100)           0         \n",
            "                                                                 \n",
            " lstm_43 (LSTM)              (None, 50, 100)           80400     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 50, 100)           0         \n",
            "                                                                 \n",
            " lstm_44 (LSTM)              (None, 50, 100)           80400     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 50, 100)           0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 50, 96)            9696      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 211,296\n",
            "Trainable params: 211,296\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 로드\n",
        "with open('test_data.txt', 'r') as f:\n",
        "    test_data = f.read()\n",
        "\n",
        "# Tokenizer를 사용하여 단어를 숫자로 변환\n",
        "test_sequences = tokenizer.texts_to_sequences([test_data])[0]\n",
        "\n",
        "# 테스트 데이터가 seq_length보다 짧을 경우 패딩\n",
        "if len(test_sequences) < seq_length:\n",
        "    test_sequences = np.pad(test_sequences, (seq_length - len(test_sequences), 0), 'constant', constant_values=0)\n",
        "\n",
        "test_sequences = tf.keras.preprocessing.sequence.pad_sequences([test_sequences], maxlen=seq_length, padding='pre')\n",
        "\n",
        "# 입력 데이터와 출력 데이터의 shape을 변경\n",
        "test_input = test_sequences[:, :seq_length, np.newaxis]\n",
        "test_target = tf.keras.utils.to_categorical(test_sequences[:, :seq_length], num_classes=96)\n",
        "print(num_classes)\n",
        "print(test_input.shape)\n",
        "print(test_target.shape)\n",
        "\n",
        "# 모델 평가\n",
        "model.evaluate(test_input, test_target, verbose=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNfYUV3ZZX8k",
        "outputId": "9eba574b-b649-4ead-ea87-c0d5c3924b43"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96\n",
            "(1, 50, 1)\n",
            "(1, 50, 96)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.133394718170166, 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 함수 정의\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        # seed_text를 숫자 시퀀스로 변환\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
        "        token_list = np.reshape(token_list, (1, max_sequence_len, 1))\n",
        "\n",
        "        # 모델 예측\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)\n",
        "        \n",
        "        # 예측 결과를 숫자에서 단어로 변환\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index in predicted[0]:\n",
        "                output_word = word\n",
        "                break\n",
        "        \n",
        "        # 다음 입력 시퀀스 생성\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# 예측 실행\n",
        "generated_text = generate_text(\"the cat\", 10, model, seq_length)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP8PkXh83Voc",
        "outputId": "a68c0f4c-b2ff-4b89-81a3-8985c040680b"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the cat domain domain domain domain domain domain domain domain domain domain\n"
          ]
        }
      ]
    }
  ]
}